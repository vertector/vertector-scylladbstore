{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScyllaDB Store - Advanced Features\n",
    "\n",
    "This notebook demonstrates all advanced production-ready features including:\n",
    "- Health checks\n",
    "- Prometheus metrics export\n",
    "- Circuit breaker pattern\n",
    "- Connection warmup\n",
    "- Metrics API\n",
    "- Advanced configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Initialize the store with advanced configuration options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import datetime, timezone\n",
    "from scylladb_store import AsyncScyllaDBStore, TTLConfig\n",
    "from cassandra.cluster import Cluster\n",
    "\n",
    "# Configuration\n",
    "CONTACT_POINTS = [\"127.0.0.1\"]\n",
    "KEYSPACE = \"advanced_features_store\"\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Store Initialization\n",
    "\n",
    "Create a store with default configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cluster and session\n",
    "cluster = Cluster(CONTACT_POINTS)\n",
    "session = await asyncio.get_event_loop().run_in_executor(None, cluster.connect)\n",
    "\n",
    "# Create store with default configuration\n",
    "store = AsyncScyllaDBStore(\n",
    "    session=session,\n",
    "    keyspace=KEYSPACE,\n",
    "    ttl=TTLConfig(refresh_on_read=True)\n",
    ")\n",
    "\n",
    "# Setup database\n",
    "await store.setup()\n",
    "print(f\"✓ Store initialized with keyspace: {KEYSPACE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Health Check\n",
    "\n",
    "Monitor the health of your ScyllaDB connection and store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform health check\n",
    "health = await store.health_check()\n",
    "\n",
    "print(f\"Overall Status: {health['status'].upper()}\")\n",
    "print(f\"Latency: {health['latency_ms']:.2f}ms\\n\")\n",
    "\n",
    "print(\"Health Checks:\")\n",
    "for check_name, result in health['checks'].items():\n",
    "    status_icon = \"✓\" if result['status'] == 'healthy' else \"✗\"\n",
    "    print(f\"  {status_icon} {check_name}: {result['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Connection Warmup\n",
    "\n",
    "Pre-establish connections for faster first requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup with 20 concurrent queries\n",
    "print(\"Starting connection warmup...\")\n",
    "await store.warmup_connections(num_queries=20)\n",
    "print(\"✓ Warmup complete - connections are ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Circuit Breaker Pattern\n",
    "\n",
    "Prevent cascading failures by enabling the circuit breaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable circuit breaker with custom thresholds\n",
    "# Note: This is a synchronous method, not async\n",
    "store.enable_circuit_breaker(\n",
    "    failure_threshold=5,      # Open circuit after 5 failures\n",
    "    success_threshold=3,      # Close after 3 successes in half-open state\n",
    "    timeout_seconds=30.0      # Wait 30s before trying again\n",
    ")\n",
    "\n",
    "print(\"✓ Circuit breaker enabled\")\n",
    "print(f\"  Failure threshold: 5\")\n",
    "print(f\"  Success threshold: 3\")\n",
    "print(f\"  Timeout: 30s\")\n",
    "\n",
    "# Check circuit breaker state (also synchronous)\n",
    "state = store.circuit_breaker.get_state()\n",
    "print(f\"\\nCircuit breaker state: {state['state']}\")\n",
    "print(f\"Failure count: {state['failure_count']}\")\n",
    "print(f\"Success count: {state['success_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metrics API\n",
    "\n",
    "Access real-time metrics about your store operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some test data to generate metrics\n",
    "test_ops = [\n",
    "    {\"namespace\": (\"metrics\", \"test\", \"1\"), \"key\": \"data\", \"value\": {\"id\": 1}},\n",
    "    {\"namespace\": (\"metrics\", \"test\", \"2\"), \"key\": \"data\", \"value\": {\"id\": 2}},\n",
    "    {\"namespace\": (\"metrics\", \"test\", \"3\"), \"key\": \"data\", \"value\": {\"id\": 3}},\n",
    "]\n",
    "\n",
    "for op in test_ops:\n",
    "    await store.aput(**op)\n",
    "\n",
    "print(\"✓ Test data created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics statistics\n",
    "metrics = await store.get_metrics()\n",
    "\n",
    "print(\"=== Query Metrics ===\")\n",
    "print(f\"Total queries: {metrics['total_queries']}\")\n",
    "print(f\"Average latency: {metrics['avg_latency_ms']:.2f}ms\")\n",
    "print(f\"Total errors: {metrics['total_errors']}\")\n",
    "print(f\"Error rate: {metrics['error_rate']:.1%}\")\n",
    "\n",
    "print(\"\\n=== Operations by Type ===\")\n",
    "for operation, count in metrics['operations'].items():\n",
    "    print(f\"  {operation}: {count}\")\n",
    "\n",
    "print(\"\\n=== Errors by Type ===\")\n",
    "if metrics['error_types']:\n",
    "    for error_type, count in metrics['error_types'].items():\n",
    "        print(f\"  {error_type}: {count}\")\n",
    "else:\n",
    "    print(\"  No errors recorded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prometheus Metrics Export\n",
    "\n",
    "Export metrics in Prometheus format for monitoring systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export metrics in Prometheus format\n",
    "prometheus_metrics = await store.export_prometheus_metrics()\n",
    "\n",
    "print(\"=== Prometheus Metrics ===\")\n",
    "print(prometheus_metrics)\n",
    "print(\"\\n✓ These metrics can be scraped by Prometheus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reset Metrics\n",
    "\n",
    "Clear accumulated metrics for a fresh start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics before reset\n",
    "before = await store.get_metrics()\n",
    "print(f\"Queries before reset: {before['total_queries']}\")\n",
    "\n",
    "# Reset metrics\n",
    "await store.reset_metrics()\n",
    "print(\"✓ Metrics reset\")\n",
    "\n",
    "# Get metrics after reset\n",
    "after = await store.get_metrics()\n",
    "print(f\"Queries after reset: {after['total_queries']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Configuration\n",
    "\n",
    "Create a store with custom connection pool and performance settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new store with advanced configuration\n",
    "# Note: Connection pool settings are configured at the Cluster level\n",
    "from cassandra.cluster import ExecutionProfile\n",
    "from cassandra.policies import DCAwareRoundRobinPolicy\n",
    "\n",
    "# Create execution profile with custom settings\n",
    "profile = ExecutionProfile(\n",
    "    load_balancing_policy=DCAwareRoundRobinPolicy(),\n",
    "    request_timeout=20.0  # 20s request timeout\n",
    ")\n",
    "\n",
    "# Create cluster with advanced configuration\n",
    "advanced_cluster = Cluster(\n",
    "    CONTACT_POINTS,\n",
    "    protocol_version=4,\n",
    "    executor_threads=8,  # More threads for async callbacks\n",
    "    compression=True,  # Enable lz4 compression\n",
    "    connect_timeout=10.0,  # 10s connect timeout\n",
    "    execution_profiles={'advanced': profile}\n",
    ")\n",
    "\n",
    "# Connect and create session\n",
    "advanced_session = await asyncio.get_event_loop().run_in_executor(\n",
    "    None, advanced_cluster.connect\n",
    ")\n",
    "\n",
    "# Create store with TTL configuration\n",
    "advanced_store = AsyncScyllaDBStore(\n",
    "    session=advanced_session,\n",
    "    keyspace=\"advanced_config_store\",\n",
    "    ttl=TTLConfig(\n",
    "        default_ttl=3600.0,      # 1 hour default TTL\n",
    "        refresh_on_read=True     # Refresh TTL on read\n",
    "    )\n",
    ")\n",
    "\n",
    "await advanced_store.setup()\n",
    "print(\"✓ Advanced store created with custom configuration\")\n",
    "print(\"  - Executor threads: 8\")\n",
    "print(\"  - Compression: lz4 enabled\")\n",
    "print(\"  - Connect timeout: 10s\")\n",
    "print(\"  - Request timeout: 20s\")\n",
    "print(\"  - Default TTL: 3600s (1 hour)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Handling\n",
    "\n",
    "Demonstrate proper error handling with custom exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scylladb_store import (\n",
    "    StoreConnectionError,\n",
    "    StoreQueryError,\n",
    "    StoreValidationError,\n",
    "    StoreTimeoutError\n",
    ")\n",
    "\n",
    "# Example 1: Validation error\n",
    "try:\n",
    "    # Try to create a namespace that's too deep\n",
    "    deep_namespace = tuple([f\"level{i}\" for i in range(20)])  # 20 levels deep\n",
    "    await store.aput(\n",
    "        namespace=deep_namespace,\n",
    "        key=\"test\",\n",
    "        value={\"data\": \"test\"}\n",
    "    )\n",
    "except StoreValidationError as e:\n",
    "    print(f\"✓ Caught validation error: {e}\")\n",
    "    print(f\"  Field: {e.field}\")\n",
    "\n",
    "# Example 2: Validation error for large value\n",
    "try:\n",
    "    # Try to store a value that's too large (>10MB)\n",
    "    large_value = {\"data\": \"x\" * (11 * 1024 * 1024)}  # 11MB\n",
    "    await store.aput(\n",
    "        namespace=(\"test\",),\n",
    "        key=\"large\",\n",
    "        value=large_value\n",
    "    )\n",
    "except StoreValidationError as e:\n",
    "    print(f\"\\n✓ Caught validation error: {e}\")\n",
    "    print(f\"  Field: {e.field}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Monitoring Slow Queries\n",
    "\n",
    "The store automatically detects and logs slow queries (>100ms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large dataset to potentially trigger slow queries\n",
    "print(\"Creating large dataset...\")\n",
    "for i in range(100):\n",
    "    await store.aput(\n",
    "        namespace=(\"performance\", \"test\"),\n",
    "        key=f\"item_{i}\",\n",
    "        value={\"id\": i, \"data\": f\"test data {i}\"}\n",
    "    )\n",
    "\n",
    "print(\"✓ Dataset created\")\n",
    "\n",
    "# Perform a search that might be slow\n",
    "print(\"\\nPerforming search...\")\n",
    "results = await store.asearch(\n",
    "    (\"performance\",),  # positional-only argument\n",
    "    limit=100\n",
    ")\n",
    "print(f\"✓ Found {len(results)} items\")\n",
    "\n",
    "# Check metrics for slow queries\n",
    "metrics = await store.get_metrics()\n",
    "print(f\"\\nAverage latency: {metrics['avg_latency_ms']:.2f}ms\")\n",
    "print(\"(Check logs for any slow query warnings >100ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Circuit Breaker in Action\n",
    "\n",
    "Demonstrate circuit breaker behavior during failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circuit breaker state tracking\n",
    "print(\"Circuit Breaker Demo\\n\")\n",
    "\n",
    "# Initial state\n",
    "state = store.circuit_breaker.get_state()\n",
    "print(f\"Initial state: {state['state']}\")\n",
    "print(f\"  Failure count: {state['failure_count']}\")\n",
    "\n",
    "# Perform some successful operations\n",
    "for i in range(5):\n",
    "    await store.aput(\n",
    "        namespace=(\"circuit\", \"test\"),\n",
    "        key=f\"item_{i}\",\n",
    "        value={\"id\": i}\n",
    "    )\n",
    "\n",
    "# Check state after successful operations\n",
    "state = store.circuit_breaker.get_state()\n",
    "print(f\"\\nAfter successful operations: {state['state']}\")\n",
    "print(f\"  Failure count: {state['failure_count']}\")\n",
    "\n",
    "# Note: Circuit breaker will open only after consecutive failures\n",
    "# In production, this prevents cascading failures when database is down\n",
    "print(\"\\n✓ Circuit breaker is monitoring all operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. TTL (Time To Live) Management\n",
    "\n",
    "Demonstrate automatic expiration of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an item with 10-second TTL\n",
    "print(\"Creating item with 10-second TTL...\")\n",
    "await store.aput(\n",
    "    namespace=(\"ttl\", \"test\"),\n",
    "    key=\"temporary\",\n",
    "    value={\"message\": \"This will expire in 10 seconds\"},\n",
    "    ttl=10.0  # TTL in seconds\n",
    ")\n",
    "\n",
    "# Verify it exists\n",
    "item = await store.aget((\"ttl\", \"test\"), \"temporary\")\n",
    "print(f\"✓ Item created: {item.value['message']}\")\n",
    "\n",
    "# Wait for expiration\n",
    "print(\"\\nWaiting 12 seconds for expiration...\")\n",
    "await asyncio.sleep(12)\n",
    "\n",
    "# Try to retrieve expired item\n",
    "expired = await store.aget((\"ttl\", \"test\"), \"temporary\")\n",
    "print(f\"Item after expiration: {expired}\")\n",
    "print(\"✓ Item expired as expected\" if expired is None else \"✗ Item still exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Execution Profiles\n",
    "\n",
    "Configure different execution settings for different query types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standard execution profiles for common use cases\n",
    "print(\"Creating standard execution profiles...\")\n",
    "store.create_standard_profiles()\n",
    "\n",
    "print(\"\\n✓ Created 4 standard profiles:\")\n",
    "print(\"  1. strong_reads    - QUORUM consistency, 30s timeout (critical reads)\")\n",
    "print(\"  2. fast_writes     - ONE consistency, 5s timeout (high throughput)\")\n",
    "print(\"  3. lwt_operations  - SERIAL consistency, 30s timeout (atomic ops)\")\n",
    "print(\"  4. analytics       - ALL consistency, 60s timeout (analytics)\")\n",
    "\n",
    "# Create a custom profile\n",
    "from cassandra.query import ConsistencyLevel\n",
    "\n",
    "store.add_execution_profile(\n",
    "    'custom_profile',\n",
    "    consistency_level=ConsistencyLevel.LOCAL_QUORUM,\n",
    "    request_timeout=15.0\n",
    ")\n",
    "print(\"\\n✓ Created custom profile: custom_profile (LOCAL_QUORUM, 15s)\")\n",
    "\n",
    "print(\"\\n✓ Execution profiles allow fine-tuned control over query execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Lightweight Transactions (LWT)\n",
    "\n",
    "Demonstrate atomic conditional operations for preventing race conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Conditional Insert (IF NOT EXISTS)\n",
    "# Useful for preventing duplicate inserts and implementing distributed locks\n",
    "\n",
    "print(\"=== Conditional Insert (IF NOT EXISTS) ===\\n\")\n",
    "\n",
    "# First attempt - should succeed\n",
    "success = await store.aput_if_not_exists(\n",
    "    namespace=(\"locks\",),\n",
    "    key=\"resource_123\",\n",
    "    value={\"owner\": \"worker_1\", \"acquired_at\": \"2025-10-03T00:00:00\"},\n",
    "    ttl=60.0\n",
    ")\n",
    "print(f\"First insert attempt: {'SUCCESS' if success else 'FAILED'}\")\n",
    "\n",
    "# Second attempt - should fail (key already exists)\n",
    "success = await store.aput_if_not_exists(\n",
    "    namespace=(\"locks\",),\n",
    "    key=\"resource_123\",\n",
    "    value={\"owner\": \"worker_2\", \"acquired_at\": \"2025-10-03T00:00:10\"}\n",
    ")\n",
    "print(f\"Second insert attempt: {'SUCCESS' if success else 'FAILED (key exists)'}\")\n",
    "\n",
    "# Verify current value\n",
    "item = await store.aget((\"locks\",), \"resource_123\")\n",
    "print(f\"\\nCurrent lock owner: {item.value['owner']}\")\n",
    "print(\"✓ Lock correctly held by first worker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Conditional Update (IF EXISTS)\n",
    "# Ensures you only update existing keys, preventing accidental creation\n",
    "\n",
    "print(\"\\n=== Conditional Update (IF EXISTS) ===\\n\")\n",
    "\n",
    "# Try to update non-existent key - should fail\n",
    "success = await store.aupdate_if_exists(\n",
    "    namespace=(\"users\",),\n",
    "    key=\"999\",\n",
    "    value={\"name\": \"Ghost User\", \"status\": \"inactive\"}\n",
    ")\n",
    "print(f\"Update non-existent user: {'SUCCESS' if success else 'FAILED (does not exist)'}\")\n",
    "\n",
    "# Create a user first\n",
    "await store.aput(\n",
    "    namespace=(\"users\",),\n",
    "    key=\"123\",\n",
    "    value={\"name\": \"Alice\", \"status\": \"active\"}\n",
    ")\n",
    "print(\"Created user 123\")\n",
    "\n",
    "# Now update - should succeed\n",
    "success = await store.aupdate_if_exists(\n",
    "    namespace=(\"users\",),\n",
    "    key=\"123\",\n",
    "    value={\"name\": \"Alice\", \"status\": \"inactive\"}\n",
    ")\n",
    "print(f\"Update existing user: {'SUCCESS' if success else 'FAILED'}\")\n",
    "\n",
    "# Verify update\n",
    "item = await store.aget((\"users\",), \"123\")\n",
    "print(f\"\\nUser 123 status: {item.value['status']}\")\n",
    "print(\"✓ Conditional update works correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Compare-And-Set (CAS)\n",
    "# Atomic read-modify-write pattern for optimistic locking\n",
    "\n",
    "print(\"\\n=== Compare-And-Set (CAS) ===\\n\")\n",
    "\n",
    "# Initialize a counter\n",
    "await store.aput(\n",
    "    namespace=(\"counters\",),\n",
    "    key=\"page_views\",\n",
    "    value={\"count\": 0}\n",
    ")\n",
    "print(\"Initialized counter at 0\")\n",
    "\n",
    "# Simulate atomic increment\n",
    "for i in range(5):\n",
    "    # Read current value\n",
    "    item = await store.aget((\"counters\",), \"page_views\")\n",
    "    current = item.value\n",
    "    \n",
    "    # Compute new value\n",
    "    new = {\"count\": current[\"count\"] + 1}\n",
    "    \n",
    "    # Atomic update only if value hasn't changed\n",
    "    success = await store.acompare_and_set(\n",
    "        namespace=(\"counters\",),\n",
    "        key=\"page_views\",\n",
    "        expected_value=current,\n",
    "        new_value=new\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(f\"Increment {i+1}: {current['count']} -> {new['count']}\")\n",
    "    else:\n",
    "        print(f\"Increment {i+1}: FAILED (concurrent modification)\")\n",
    "\n",
    "# Verify final count\n",
    "item = await store.aget((\"counters\",), \"page_views\")\n",
    "print(f\"\\nFinal count: {item.value['count']}\")\n",
    "print(\"✓ CAS ensures atomicity even with concurrent updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Conditional Delete\n",
    "# Delete only if conditions are met\n",
    "\n",
    "print(\"\\n=== Conditional Delete ===\\n\")\n",
    "\n",
    "# Create test items\n",
    "await store.aput((\"temp\",), \"item1\", {\"status\": \"active\"})\n",
    "await store.aput((\"temp\",), \"item2\", {\"status\": \"active\"})\n",
    "print(\"Created 2 test items\")\n",
    "\n",
    "# Try to delete non-existent item - should fail\n",
    "success = await store.adelete_if_exists((\"temp\",), \"item999\")\n",
    "print(f\"\\nDelete non-existent item: {'SUCCESS' if success else 'FAILED (does not exist)'}\")\n",
    "\n",
    "# Delete existing item - should succeed\n",
    "success = await store.adelete_if_exists((\"temp\",), \"item1\")\n",
    "print(f\"Delete existing item1: {'SUCCESS' if success else 'FAILED'}\")\n",
    "\n",
    "# Delete with value check - wrong value should fail\n",
    "success = await store.adelete_if_value(\n",
    "    namespace=(\"temp\",),\n",
    "    key=\"item2\",\n",
    "    expected_value={\"status\": \"inactive\"}\n",
    ")\n",
    "print(f\"Delete item2 with wrong value: {'SUCCESS' if success else 'FAILED (value mismatch)'}\")\n",
    "\n",
    "# Delete with correct value - should succeed\n",
    "success = await store.adelete_if_value(\n",
    "    namespace=(\"temp\",),\n",
    "    key=\"item2\",\n",
    "    expected_value={\"status\": \"active\"}\n",
    ")\n",
    "print(f\"Delete item2 with correct value: {'SUCCESS' if success else 'FAILED'}\")\n",
    "\n",
    "# Verify deletions\n",
    "item1 = await store.aget((\"temp\",), \"item1\")\n",
    "item2 = await store.aget((\"temp\",), \"item2\")\n",
    "print(f\"\\nitem1 exists: {item1 is not None}\")\n",
    "print(f\"item2 exists: {item2 is not None}\")\n",
    "print(\"✓ Conditional deletes work correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Distributed Lock Pattern\n",
    "# Real-world use case: coordinating work across multiple workers\n",
    "\n",
    "print(\"\\n=== Distributed Lock Pattern ===\\n\")\n",
    "\n",
    "async def acquire_lock(resource_id: str, worker_id: str, ttl: float = 30.0) -> bool:\n",
    "    \"\"\"Try to acquire a distributed lock.\"\"\"\n",
    "    return await store.aput_if_not_exists(\n",
    "        namespace=(\"locks\",),\n",
    "        key=resource_id,\n",
    "        value={\"owner\": worker_id, \"acquired_at\": datetime.now(timezone.utc).isoformat()},\n",
    "        ttl=ttl\n",
    "    )\n",
    "\n",
    "async def release_lock(resource_id: str, worker_id: str) -> bool:\n",
    "    \"\"\"Release a lock only if we own it.\"\"\"\n",
    "    item = await store.aget((\"locks\",), resource_id)\n",
    "    if item and item.value.get(\"owner\") == worker_id:\n",
    "        return await store.adelete_if_value(\n",
    "            namespace=(\"locks\",),\n",
    "            key=resource_id,\n",
    "            expected_value=item.value\n",
    "        )\n",
    "    return False\n",
    "\n",
    "# Simulate multiple workers trying to acquire lock\n",
    "print(\"Worker 1 acquiring lock...\")\n",
    "lock1 = await acquire_lock(\"task_processor\", \"worker_1\")\n",
    "print(f\"Worker 1: {'✓ Lock acquired' if lock1 else '✗ Failed'}\")\n",
    "\n",
    "print(\"\\nWorker 2 trying to acquire same lock...\")\n",
    "lock2 = await acquire_lock(\"task_processor\", \"worker_2\")\n",
    "print(f\"Worker 2: {'✓ Lock acquired' if lock2 else '✗ Failed (already locked)'}\")\n",
    "\n",
    "# Worker 2 cannot release Worker 1's lock\n",
    "print(\"\\nWorker 2 trying to release Worker 1's lock...\")\n",
    "released = await release_lock(\"task_processor\", \"worker_2\")\n",
    "print(f\"Worker 2 release: {'✓ Success' if released else '✗ Failed (not owner)'}\")\n",
    "\n",
    "# Worker 1 can release its own lock\n",
    "print(\"\\nWorker 1 releasing its lock...\")\n",
    "released = await release_lock(\"task_processor\", \"worker_1\")\n",
    "print(f\"Worker 1 release: {'✓ Success' if released else '✗ Failed'}\")\n",
    "\n",
    "# Now Worker 2 can acquire\n",
    "print(\"\\nWorker 2 trying again...\")\n",
    "lock2 = await acquire_lock(\"task_processor\", \"worker_2\")\n",
    "print(f\"Worker 2: {'✓ Lock acquired' if lock2 else '✗ Failed'}\")\n",
    "\n",
    "print(\"\\n✓ Distributed locking prevents race conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. ScyllaDB Shard Awareness\n",
    "\n",
    "Check ScyllaDB-specific shard-aware optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ScyllaDB shard awareness information\n",
    "shard_info = store.get_shard_awareness_info()\n",
    "\n",
    "print(\"=== ScyllaDB Shard Awareness ===\\n\")\n",
    "\n",
    "if shard_info['is_shard_aware']:\n",
    "    print(\"✓ Shard awareness: ENABLED\")\n",
    "    print(\"  This optimizes performance by routing queries directly to the correct shard\")\n",
    "    print(\"  Reduces latency by eliminating inter-shard communication\")\n",
    "    \n",
    "    if shard_info['shard_stats']:\n",
    "        print(f\"\\n  Shard statistics:\")\n",
    "        print(f\"  {shard_info['shard_stats']}\")\n",
    "else:\n",
    "    print(\"✗ Shard awareness: NOT AVAILABLE\")\n",
    "    print(\"  (This is a ScyllaDB-specific optimization)\")\n",
    "\n",
    "print(f\"\\n=== Cluster Metadata ===\")\n",
    "print(f\"Contact points: {shard_info['cluster_metadata']['contact_points']}\")\n",
    "print(f\"Protocol version: {shard_info['cluster_metadata']['protocol_version']}\")\n",
    "print(f\"Compression: {shard_info['cluster_metadata']['compression']}\")\n",
    "\n",
    "print(\"\\n✓ ScyllaDB-specific optimizations configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Query Paging for Large Result Sets\n",
    "\n",
    "Efficiently handle large result sets with automatic paging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset to demonstrate paging\n",
    "print(\"Creating dataset with 500 items...\")\n",
    "for i in range(500):\n",
    "    await store.aput(\n",
    "        namespace=(\"paging\", \"test\"),\n",
    "        key=f\"item_{i:04d}\",\n",
    "        value={\"id\": i, \"data\": f\"test data {i}\"}\n",
    "    )   \n",
    "print(\"✓ Dataset created\\n\")\n",
    "# Search with default paging (fetch_size=5000)\n",
    "print(\"=== Search with Default Paging (fetch_size=5000) ===\")\n",
    "results = await store.asearch((\"paging\",), limit=500)\n",
    "print(f\"Retrieved {len(results)} items\")\n",
    "print(\"✓ All items fit in single page (500 < 5000)\\n\")\n",
    "# Search with small page size (fetch_size=50)\n",
    "print(\"=== Search with Small Page Size (fetch_size=50) ===\")\n",
    "results = await store.asearch(\n",
    "    (\"paging\",),\n",
    "    limit=500,\n",
    "    fetch_size=50  # Fetch 50 rows per page\n",
    ")\n",
    "print(f\"Retrieved {len(results)} items\")\n",
    "print(\"✓ Driver automatically fetched 10 pages (50 rows each)\")\n",
    "print(\"✓ Reduces memory usage - good for memory-constrained environments\\n\")\n",
    "# Search with medium page size\n",
    "print(\"=== Search with Medium Page Size (fetch_size=200) ===\")\n",
    "results = await store.asearch(\n",
    "    (\"paging\",),\n",
    "    limit=500,\n",
    "    fetch_size=200\n",
    ")\n",
    "print(f\"Retrieved {len(results)} items\")\n",
    "print(\"✓ Driver fetched 3 pages (200 rows each)\")\n",
    "print(\"✓ Balanced approach - fewer round trips than fetch_size=50\\n\")\n",
    "print(\"=== Paging Trade-offs ===\")\n",
    "print(\"  Small fetch_size (e.g., 50):   Less memory, more round trips\")\n",
    "print(\"  Medium fetch_size (e.g., 200): Balanced memory and round trips\")\n",
    "print(\"  Large fetch_size (e.g., 5000): More memory, fewer round trips\")\n",
    "print(\"  Default (5000):                Good balance for most use cases\")\n",
    "print(\"\\n✓ Choose based on: dataset size, memory constraints, latency requirements\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Atomic Batch Operations\n",
    "\n",
    "Demonstrate true atomic batch processing with LOGGED and UNLOGGED batch types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: UNLOGGED Batch (Default - Best Performance)\n",
    "print(\"=== UNLOGGED Batch (Default) ===\\n\")\n",
    "\n",
    "from scylladb_store import PutOp\n",
    "\n",
    "# Create batch of PUT operations\n",
    "ops = [\n",
    "    PutOp(namespace=('batch', 'unlogged'), key=f'item_{i}', value={'id': i, 'data': f'test {i}'})\n",
    "    for i in range(10)\n",
    "]\n",
    "\n",
    "# Execute with default UNLOGGED batch\n",
    "results = await store.abatch(ops)\n",
    "print(f\"✓ Executed {len(ops)} operations atomically\")\n",
    "print(f\"  Batch type: UNLOGGED (best performance)\")\n",
    "print(f\"  Atomicity: Within partition\")\n",
    "\n",
    "# Verify data\n",
    "item = await store.aget(('batch', 'unlogged'), 'item_5')\n",
    "print(f\"  Verified item_5: {item.value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: LOGGED Batch (Full Atomicity)\n",
    "print(\"\\n=== LOGGED Batch (Full Atomicity) ===\\n\")\n",
    "\n",
    "# Create batch for critical operations requiring full atomicity\n",
    "ops_logged = [\n",
    "    PutOp(namespace=('batch', 'logged'), key=f'critical_{i}', value={'id': i, 'type': 'critical'})\n",
    "    for i in range(5)\n",
    "]\n",
    "\n",
    "# Execute with LOGGED batch for full atomicity\n",
    "results = await store.abatch(ops_logged, batch_type='LOGGED')\n",
    "print(f\"✓ Executed {len(ops_logged)} operations atomically\")\n",
    "print(f\"  Batch type: LOGGED (full atomicity across partitions)\")\n",
    "print(f\"  Use case: Financial transactions, critical updates\")\n",
    "\n",
    "# Verify data\n",
    "item = await store.aget(('batch', 'logged'), 'critical_2')\n",
    "print(f\"  Verified critical_2: {item.value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Batch with Retry Logic\n",
    "print(\"\\n=== Batch with Retry Logic ===\\n\")\n",
    "\n",
    "# Create batch with retry configuration\n",
    "ops_retry = [\n",
    "    PutOp(namespace=('batch', 'retry'), key=f'item_{i}', value={'id': i, 'resilient': True})\n",
    "    for i in range(10)\n",
    "]\n",
    "\n",
    "# Execute with retry configuration for resilience\n",
    "results = await store.abatch(\n",
    "    ops_retry,\n",
    "    max_retries=3,          # Retry up to 3 times\n",
    "    retry_delay=0.1,        # Start with 100ms delay\n",
    "    retry_backoff=2.0       # Double delay after each retry\n",
    ")\n",
    "\n",
    "print(f\"✓ Executed {len(results)} operations with retry protection\")\n",
    "print(f\"  Retry configuration:\")\n",
    "print(f\"    - Max retries: 3\")\n",
    "print(f\"    - Initial delay: 0.1s\")\n",
    "print(f\"    - Backoff: 2.0x (exponential)\")\n",
    "print(f\"  Delays: 0.1s → 0.2s → 0.4s\")\n",
    "print(f\"  Retryable errors: Timeout, Unavailable replicas\")\n",
    "\n",
    "print(\"\\n✓ Batch operations are resilient to transient failures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Mixed Operations (Falls back to Concurrent Execution)\n",
    "print(\"\\n=== Mixed Operations Batch ===\\n\")\n",
    "\n",
    "from scylladb_store import GetOp\n",
    "\n",
    "# Create batch with mixed operation types\n",
    "mixed_ops = [\n",
    "    PutOp(namespace=('batch', 'mixed'), key='new_item', value={'type': 'put'}),\n",
    "    GetOp(namespace=('batch', 'unlogged'), key='item_5'),\n",
    "    PutOp(namespace=('batch', 'mixed'), key='another', value={'type': 'put2'}),\n",
    "]\n",
    "\n",
    "# Execute - automatically uses concurrent execution for mixed operations\n",
    "results = await store.abatch(mixed_ops)\n",
    "print(f\"✓ Executed {len(mixed_ops)} operations\")\n",
    "print(f\"  Mode: Concurrent (mixed operation types)\")\n",
    "print(f\"  Note: batch_type parameter ignored for mixed operations\")\n",
    "print(f\"  GET result: {results[1].value if results[1] else None}\")\n",
    "\n",
    "print(\"\\n✓ abatch() intelligently selects atomic vs concurrent execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Batch with TTL\n",
    "print(\"\\n=== Batch with TTL ===\\n\")\n",
    "\n",
    "# Create batch with TTL for temporary data\n",
    "ops_ttl = [\n",
    "    PutOp(namespace=('batch', 'ttl'), key=f'temp_{i}', value={'id': i}, ttl=60.0)\n",
    "    for i in range(5)\n",
    "]\n",
    "\n",
    "# Execute batch with TTL\n",
    "results = await store.abatch(ops_ttl)\n",
    "print(f\"✓ Executed {len(ops_ttl)} operations with 60s TTL\")\n",
    "print(f\"  All items will expire in 60 seconds\")\n",
    "\n",
    "# Verify data\n",
    "item = await store.aget(('batch', 'ttl'), 'temp_0')\n",
    "print(f\"  Verified temp_0: {item.value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Comprehensive Health Check\n",
    "\n",
    "Perform a final health check to verify all systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive health check\n",
    "health = await store.health_check()\n",
    "\n",
    "print(\"=== Final Health Check ===\")\n",
    "print(f\"Overall Status: {health['status'].upper()}\")\n",
    "print(f\"Check Latency: {health['latency_ms']:.2f}ms\\n\")\n",
    "\n",
    "for check_name, result in health['checks'].items():\n",
    "    status_icon = \"✓\" if result['status'] == 'healthy' else \"✗\"\n",
    "    print(f\"{status_icon} {check_name.upper()}\")\n",
    "    print(f\"  {result['message']}\")\n",
    "    if 'details' in result:\n",
    "        for key, value in result['details'].items():\n",
    "            print(f\"  - {key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Final Metrics Summary\n",
    "\n",
    "Review all metrics collected during this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final metrics\n",
    "metrics = await store.get_metrics()\n",
    "\n",
    "print(\"=== Final Metrics Summary ===\")\n",
    "print(f\"\\nTotal Operations: {metrics['total_queries']}\")\n",
    "print(f\"Average Latency: {metrics['avg_latency_ms']:.2f}ms\")\n",
    "print(f\"Error Rate: {metrics['error_rate']:.2%}\")\n",
    "\n",
    "print(\"\\nOperations Breakdown:\")\n",
    "for op_type, count in sorted(metrics['operations'].items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {op_type}: {count}\")\n",
    "\n",
    "if metrics['error_types']:\n",
    "    print(\"\\nErrors Encountered:\")\n",
    "    for error_type, count in metrics['error_types'].items():\n",
    "        print(f\"  {error_type}: {count}\")\n",
    "else:\n",
    "    print(\"\\n✓ No errors encountered\")\n",
    "\n",
    "print(f\"\\n✓ All advanced features demonstrated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Cleanup (Optional)\n",
    "\n",
    "Clean up test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up test data\n",
    "async def cleanup():\n",
    "    namespaces = await store.alist_namespaces()\n",
    "    \n",
    "    for ns in namespaces:\n",
    "        items = await store.asearch(ns, limit=1000)\n",
    "        for item in items:\n",
    "            await store.adelete(item.namespace, item.key)\n",
    "    \n",
    "    print(\"✓ All test data cleaned up\")\n",
    "\n",
    "await cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✓ **Health Checks** - Monitor store and database health\n",
    "2. ✓ **Connection Warmup** - Pre-establish connections for better performance\n",
    "3. ✓ **Circuit Breaker** - Prevent cascading failures\n",
    "4. ✓ **Metrics API** - Real-time operation statistics\n",
    "5. ✓ **Prometheus Export** - Integration with monitoring systems\n",
    "6. ✓ **Advanced Configuration** - Custom connection pools and settings\n",
    "7. ✓ **Error Handling** - Comprehensive exception hierarchy\n",
    "8. ✓ **Slow Query Detection** - Automatic performance monitoring\n",
    "9. ✓ **TTL Management** - Automatic data expiration\n",
    "10. ✓ **Production-Ready** - All best practices implemented\n",
    "\n",
    "### Key Features for Production:\n",
    "\n",
    "- **Native Async I/O**: True asyncio integration with AsyncioConnection\n",
    "- **Observability**: Comprehensive metrics and health checks\n",
    "- **Resilience**: Circuit breaker pattern prevents cascading failures\n",
    "- **Performance**: Connection warmup, compression, optimized connection pools\n",
    "- **Monitoring**: Prometheus metrics export for production monitoring\n",
    "- **Error Handling**: Rich exception hierarchy with context\n",
    "- **Validation**: Input validation prevents data corruption\n",
    "- **Flexibility**: Highly configurable for different workloads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "databases",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
